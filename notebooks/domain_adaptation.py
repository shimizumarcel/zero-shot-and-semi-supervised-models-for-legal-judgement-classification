# -*- coding: utf-8 -*-
"""domain_adaptation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-blPK_trpHUVugW9kJVs8Z4ylqA75t4V
"""

!pip install -U datasets
!pip install optuna
!pip install python-dotenv

from google.colab import drive
import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer, DataCollatorForLanguageModeling, set_seed, AutoModelForMaskedLM, TrainingArguments, Trainer, AutoConfig, AutoModelForSequenceClassification
import torch
from huggingface_hub import login
import numpy as np
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import os
os.environ["WANDB_DISABLED"] = "true"
import optuna


from dotenv import load_dotenv
import os

load_dotenv()
login(token=os.getenv("HUGGINGFACE_TOKEN"))

drive.mount('/content/drive')

df_train = pd.read_csv("/content/drive/My Drive/tcc/data/train.csv")
train, valid = train_test_split(df_train, test_size = 0.5)

test = pd.read_csv("/content/drive/My Drive/tcc/data/test.csv")
unsup = pd.read_csv("/content/drive/My Drive/tcc/data/unsup.csv")

ds = DatasetDict({
    "train": Dataset.from_pandas(train.reset_index(drop = True)),
    "valid": Dataset.from_pandas(valid.reset_index(drop = True)),
    "test": Dataset.from_pandas(test.reset_index(drop = True)),
    "unsup": Dataset.from_pandas(unsup.reset_index(drop = True)),
})

tokenizer = AutoTokenizer.from_pretrained("neuralmind/bert-base-portuguese-cased")

def tokenize(batch):
  return tokenizer(batch["text"], truncation = True, max_length = 128, return_special_tokens_mask = True)

ds_mlm = ds.map(tokenize, batched = True)

ds_mlm = ds_mlm.remove_columns(["label", "text"])

data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm_probability = 0.15)

training_args = TrainingArguments(
    output_dir = "/content/drive/My Drive/tcc/domain_adaptation/training_args",
    per_device_train_batch_size = 32,
    num_train_epochs = 6,
    logging_strategy = "epoch",
    evaluation_strategy = "epoch",
    save_strategy  = "no",
    log_level = "error",
    report_to = "none"
)

trainer = Trainer(
    model = AutoModelForMaskedLM.from_pretrained("neuralmind/bert-base-portuguese-cased"), processing_class = tokenizer, args = training_args,
    data_collator = data_collator, train_dataset = ds_mlm["unsup"], eval_dataset = ds_mlm["train"]
)

trainer.train()

trainer.model.save_pretrained("/content/drive/My Drive/tcc/domain_adaptation")
tokenizer.save_pretrained("/content/drive/My Drive/tcc/domain_adaptation")

def tonekize(batch):
  return tokenizer(batch["text"], truncation = True, max_length = 128)

ds_enc = ds.map(tokenize, batched = True)
ds_enc = ds_enc.remove_columns(["text"])

model = AutoModelForSequenceClassification.from_pretrained("/content/drive/My Drive/tcc/domain_adaptation")

def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    accuracy = accuracy_score(labels, preds)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="macro")
    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1": f1,
    }

config = AutoConfig.from_pretrained("/content/drive/My Drive/tcc/domain_adaptation")
config.num_labels = 2

def objective(trial):
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 5e-5, log=True)
    num_train_epochs = trial.suggest_int("num_train_epochs", 3, 10)
    weight_decay = trial.suggest_categorical("weight_decay", [0.0, 0.1, 0.01])
    lr_scheduler_type = trial.suggest_categorical("lr_scheduler_type", ["linear", "constant", "constant_with_warmup"])

    training_args = TrainingArguments(
        output_dir = "/content/drive/My Drive/tcc/fine_tuning",
        num_train_epochs=3,
        learning_rate = learning_rate,
        lr_scheduler_type = lr_scheduler_type,
        per_device_train_batch_size = 32,
        per_device_eval_batch_size = 32,
        evaluation_strategy = "epoch",
        save_strategy = "epoch",
        weight_decay = weight_decay,
        logging_strategy = "epoch",
        load_best_model_at_end = True,
        metric_for_best_model = "eval_accuracy",
        save_total_limit = 1,
        log_level = "error",
        report_to = "none",
    )
    trainer = Trainer(
        model = model,
        processing_class = tokenizer,
        args = training_args,
        compute_metrics = compute_metrics,
        train_dataset = ds_enc["train"],
        eval_dataset = ds_enc["valid"],
        )

    trainer.train()
    metrics = trainer.evaluate()
    return metrics["eval_accuracy"]

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=10)


print("Melhores Hiperpar√¢metros:")
print(study.best_params)

training_args_fine_tune = TrainingArguments(
    output_dir = "/content/drive/My Drive/tcc/fine_tuning",
    num_train_epochs = 6,
    learning_rate = 3.415422640696538e-05,
    lr_scheduler_type = "constant_with_warmup",
    per_device_train_batch_size = 4,
    per_device_eval_batch_size=32,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    weight_decay= 0.0,
    logging_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_accuracy",
    save_total_limit=1, log_level='error')

trainer = Trainer(
    model=model,
    processing_class =tokenizer,
    args=training_args_fine_tune,
    compute_metrics=compute_metrics,
    train_dataset=ds_enc["train"],
    eval_dataset=ds_enc["valid"],
)

trainer.train()

pred = trainer.predict(ds_enc["test"])
metrics = compute_metrics(pred)
print(f"Metrics on test set: {metrics}")

model_no_adapt = AutoModelForSequenceClassification.from_pretrained("neuralmind/bert-base-portuguese-cased", config=config)

trainer_no_adapt = Trainer(
    model = model_no_adapt,
    processing_class = tokenizer,
    args = training_args_fine_tune,
    compute_metrics = compute_metrics,
    train_dataset=ds_enc["train"],
    eval_dataset=ds_enc["valid"],
)

trainer_no_adapt.train()

pred_no_adapt = trainer_no_adapt.predict(ds_enc["test"])
metrics_no_adapt = compute_metrics(pred_no_adapt)
print(f"Metrics without Domain Adaptation: {metrics_no_adapt}")

